# 03.21 ğŸ“
## ë‹¤ì¤‘ í¼ì…‰íŠ¸ë¡ 
* Hidden Layerê°€ í¬í•¨ëœ ì‹¬ì¸µ ì‹ ê²½ìƒ
* Hidden Layer
â†³ Weight Sum -> Activation function
* ì‹¬ì¸µ ì‹ ê²½ë§ í•™ìŠµë²•
â†³ 1) Feed Forward ìˆ˜í–‰ (ì•ì—ì„œë¶€í„°)
  2) Backpropagation ìˆ˜í–‰ (ë’¤ì—ì„œë¶€í„°)
  3) Weight update
  4) 1~3 ê³¼ì •ì„ iteration ë§Œí¼ ìˆ˜í–‰
 ## Backpropagation
 * Outputë¶€í„° ì—­ìˆœìœ¼ë¡œ gradientë¥¼ ì „ë‹¬í•˜ì—¬ ì „ì²´ layer ê°€ì¤‘ì¹˜ë¥¼ update
 * ![](https://i.imgur.com/kARezeq.png)
 Ref. https://www.jeremyjordan.me/neural-networks-training/
 ## Activation Function
 * Sigmoid, Hyperbolic Tangent, ReLu
 * ë¹„ì„ í˜•ì„±ì„ ì ìš©í•˜ê¸° ìœ„í•¨
 * Sigmoid : Binary Classification (0, 1 ë°˜í™˜)
 * Softmax : Multi Classification
 ## Vanishing Gradient
 * ì˜¤ì°¨ê°€ í¬ê²Œ ì¤„ì–´ë“¤ì–´ í•™ìŠµì´ ë˜ì§€ ì•ŠëŠ” í˜„ìƒ
 â†³ weightê°€ ë”ì´ìƒ update X
 ## Softmax
 * Multi Classificationì˜ ìµœì¢… í™œì„±í™” í•¨ìˆ˜ë¡œ ì‚¬ìš©
 * ![](https://i.imgur.com/CO8CF95.png)
Ref. https://wikidocs.net/35476
* argmax -> ê°€ì¥ í° ê°’ì˜ Indexë¥¼ ë°˜í™˜
## Optimizer
* ìµœì ìœ¼ë¡œ graident descent ì ìš©
* Momentum, AdaGrad, RMSProp, ADAM
## Momentum
* weightë¥¼ updateí•  ë•Œë§ˆë‹¤ ì´ì „ì˜ graidentë“¤ì˜ ê°’ì„ ì¼ì • ìˆ˜ì¤€ ë°˜ì˜
* ![](https://i.imgur.com/Wrio0DC.png)
## AdaGrad
* weight ë³„ë¡œ ì„œë¡œ ë‹¤ë¥¸ Learning Rate ì ìš©
* **ì ê²Œ** ë³€í™”ëœ weightì—ëŠ” **í°** Learning Rate ì ìš©
* **í¬ê²Œ** ë³€í™”ëœ weightì—ëŠ” **ì‘ì€** Learning Rate ì ìš©
* Learning Rate ê°’ì´ ì•„ì£¼ ì‘ê²Œ ë³€í™˜ë˜ëŠ” ë¬¸ì œì 