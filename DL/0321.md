# 03.21 📝
## 다중 퍼셉트론
* Hidden Layer가 포함된 심층 신경상
* Hidden Layer
↳ Weight Sum -> Activation function
* 심층 신경망 학습법
↳ 1) Feed Forward 수행 (앞에서부터)
  2) Backpropagation 수행 (뒤에서부터)
  3) Weight update
  4) 1~3 과정을 iteration 만큼 수행
 ## Backpropagation
 * Output부터 역순으로 gradient를 전달하여 전체 layer 가중치를 update
 * ![](https://i.imgur.com/kARezeq.png)
 Ref. https://www.jeremyjordan.me/neural-networks-training/
 ## Activation Function
 * Sigmoid, Hyperbolic Tangent, ReLu
 * 비선형성을 적용하기 위함
 * Sigmoid : Binary Classification (0, 1 반환)
 * Softmax : Multi Classification
 ## Vanishing Gradient
 * 오차가 크게 줄어들어 학습이 되지 않는 현상
 ↳ weight가 더이상 update X
 ## Softmax
 * Multi Classification의 최종 활성화 함수로 사용
 * ![](https://i.imgur.com/CO8CF95.png)
Ref. https://wikidocs.net/35476
* argmax -> 가장 큰 값의 Index를 반환
## Optimizer
* 최적으로 graident descent 적용
* Momentum, AdaGrad, RMSProp, ADAM
## Momentum
* weight를 update할 때마다 이전의 graident들의 값을 일정 수준 반영
* ![](https://i.imgur.com/Wrio0DC.png)
## AdaGrad
* weight 별로 서로 다른 Learning Rate 적용
* **적게** 변화된 weight에는 **큰** Learning Rate 적용
* **크게** 변화된 weight에는 **작은** Learning Rate 적용
* Learning Rate 값이 아주 작게 변환되는 문제점